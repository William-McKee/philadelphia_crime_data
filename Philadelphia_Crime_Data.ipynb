{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philadelphia Crime Data Project\n",
    "#### William McKee\n",
    "#### September 2017\n",
    "\n",
    "In this investigation, I am analyzing Philadelphia Crime Statistics from 2006 through 2015.  The data set is available on Kaggle.  I would like to know the following about crime in my home city:\n",
    "\n",
    "(1) What parts of the city have the most crime?\n",
    "\n",
    "(2) What crimes are most frequently committed?\n",
    "\n",
    "(3) Has crime improved over time?\n",
    "\n",
    "I will look at combinations to see how crime varies geographically in Philadelphia.  Perhaps thefts and murders are committed more frequently in different parts of the city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Basic Statistics\n",
    "\n",
    "The investigation begins by gathering basic data about Philadelphia crime statistics.  Below, we can see the number of entries in the data set, which is slightly above 2.2 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "(2237605, 14)\n"
     ]
    }
   ],
   "source": [
    "# Initial library declarations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Store Philadelphia's crime data\n",
    "crime_data = pd.read_csv('crime.csv')\n",
    "\n",
    "# How large is data set?\n",
    "print(\"Data dimensions: \")\n",
    "print(crime_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are able to see the names of our 14 columns, and get some basic information about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column values:\n",
      "['Dc_Dist' 'Psa' 'Dispatch_Date_Time' 'Dispatch_Date' 'Dispatch_Time'\n",
      " 'Hour' 'Dc_Key' 'Location_Block' 'UCR_General' 'Text_General_Code'\n",
      " 'Police_Districts' 'Month' 'Lon' 'Lat']\n",
      "\n",
      "\n",
      "Column max:\n",
      "Dc_Dist                                92\n",
      "Psa                                     Z\n",
      "Dispatch_Date_Time    2017-03-23 01:29:00\n",
      "Dispatch_Date                  2017-03-23\n",
      "Dispatch_Time                    23:59:00\n",
      "Hour                                   23\n",
      "Dc_Key                       201777001445\n",
      "Location_Block              `732 SIGEL ST\n",
      "UCR_General                          2600\n",
      "Police_Districts                       22\n",
      "Month                             2017-03\n",
      "Lon                              -74.9575\n",
      "Lat                               40.1379\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Column min:\n",
      "Dc_Dist                                       1\n",
      "Psa                                           1\n",
      "Dispatch_Date_Time          2006-01-01 00:00:00\n",
      "Dispatch_Date                        2006-01-01\n",
      "Dispatch_Time                          00:00:00\n",
      "Hour                                          0\n",
      "Dc_Key                             199812085407\n",
      "Location_Block        \"A\" ST  & WESTMORELAND ST\n",
      "UCR_General                                 100\n",
      "Police_Districts                              1\n",
      "Month                                   2006-01\n",
      "Lon                                    -75.2777\n",
      "Lat                                       39.87\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Explore column names, max values, and min values\n",
    "print(\"Column values:\")\n",
    "print(crime_data.columns.values)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Column max:\")\n",
    "print(crime_data.max())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Column min:\")\n",
    "print(crime_data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, some data columns are explored, such as police districts, crime codes, and crime descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dc_Dist\n",
      "1      48815\n",
      "2     116500\n",
      "3      85816\n",
      "4      29198\n",
      "5      31433\n",
      "6      96568\n",
      "7      44687\n",
      "8      73820\n",
      "9      84046\n",
      "12    132145\n",
      "14    120931\n",
      "15    184677\n",
      "16     73052\n",
      "17     74900\n",
      "18    109746\n",
      "19    138987\n",
      "22    127332\n",
      "23     27278\n",
      "24    161909\n",
      "25    151245\n",
      "26     86982\n",
      "35    131037\n",
      "39     97061\n",
      "77      7813\n",
      "92      1627\n",
      "Name: Dc_Dist, dtype: int64\n",
      "\n",
      "\n",
      "UCR_General  Text_General_Code                      \n",
      "100.0        Homicide - Criminal                          3442\n",
      "             Homicide - Gross Negligence                    12\n",
      "             Homicide - Justifiable                         42\n",
      "200.0        Rape                                        11852\n",
      "300.0        Robbery Firearm                             40577\n",
      "             Robbery No Firearm                          51919\n",
      "400.0        Aggravated Assault Firearm                  27934\n",
      "             Aggravated Assault No Firearm               68989\n",
      "500.0        Burglary Non-Residential                    23276\n",
      "             Burglary Residential                        94143\n",
      "600.0        Theft from Vehicle                         171135\n",
      "             Thefts                                     257923\n",
      "700.0        Motor Vehicle Theft                         46517\n",
      "             Recovered Stolen Motor Vehicle              95282\n",
      "800.0        Other Assaults                             277332\n",
      "900.0        Arson                                        5684\n",
      "1000.0       Forgery and Counterfeiting                   4843\n",
      "1100.0       Fraud                                      114416\n",
      "1200.0       Embezzlement                                 4807\n",
      "1300.0       Receiving Stolen Property                     786\n",
      "1400.0       Vandalism/Criminal Mischief                200345\n",
      "1500.0       Weapon Violations                           19092\n",
      "1600.0       Prostitution and Commercialized Vice        12854\n",
      "1700.0       Other Sex Offenses (Not Commercialized)     15304\n",
      "1800.0       Narcotic / Drug Law Violations             137448\n",
      "1900.0       Gambling Violations                           921\n",
      "2000.0       Offenses Against Family and Children         1794\n",
      "2100.0       DRIVING UNDER THE INFLUENCE                 53721\n",
      "2200.0       Liquor Law Violations                        5439\n",
      "2300.0       Public Drunkenness                           4619\n",
      "2400.0       Disorderly Conduct                          40137\n",
      "2500.0       Vagrancy/Loitering                           6776\n",
      "2600.0       All Other Offenses                         437581\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Look at some distinct data\n",
    "dc_dist_distinct = crime_data.groupby('Dc_Dist')['Dc_Dist'].count()\n",
    "print(dc_dist_distinct)\n",
    "print(\"\\n\")\n",
    "\n",
    "ucr_text_distinct = crime_data.groupby(['UCR_General', 'Text_General_Code']).size()\n",
    "print(ucr_text_distinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I seen that the date and time entries generally had certain patterns.  This code will tell us if there is any data that does not have the correct pattern which would need cleaning.  Good news - there are no incorrectly formatted date and time entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do all elements in columns match expected pattern?\n",
      "Dispatch_Date_Time:  True\n",
      "Dispatch_Date:  True\n",
      "Dispatch_Time:  True\n",
      "Hour:  True\n",
      "Month:  True\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", 'This pattern has match groups')\n",
    "\n",
    "# Check dates and times for consistent formatting\n",
    "def does_column_match_pattern(data_frame, column_name, pattern):\n",
    "    ''' Returns true if every value of a data frame column matches a pattern\n",
    "        data_frame = the pandas data frame to be checked\n",
    "        column_name = name of column to be checked\n",
    "        pattern = regular expression to be matched\n",
    "    '''\n",
    "    data = data_frame[column_name]\n",
    "    return data.str.contains(pattern).all()\n",
    "\n",
    "# Patterns copied from http://regexlib.com/DisplayPatterns.aspx?cattabindex=4&categoryId=5&AspxAutoDetectCookieSupport=1\n",
    "date_time_re = '20\\d{2}-((0[1-9])|(1[0-2]))-((0[1-9])|([1-2][0-9])|(3[0-1]))(\\s)(([0-1][0-9])|(2[0-3])):([0-5][0-9]):([0-5][0-9])'\n",
    "date_re = '20\\d{2}-((0[1-9])|(1[0-2]))-((0[1-9])|([1-2][0-9])|(3[0-1]))'\n",
    "time_re = '^(([0-1]?[0-9])|([2][0-3])):([0-5]?[0-9])(:([0-5]?[0-9]))?$'\n",
    "print(\"Do all elements in columns match expected pattern?\")\n",
    "print(\"Dispatch_Date_Time: \", does_column_match_pattern(crime_data, 'Dispatch_Date_Time', date_time_re))\n",
    "print(\"Dispatch_Date: \", does_column_match_pattern(crime_data, 'Dispatch_Date', date_re))\n",
    "print(\"Dispatch_Time: \", does_column_match_pattern(crime_data, 'Dispatch_Time', time_re))\n",
    "print(\"Hour: \", ((crime_data['Hour'] >= 0) & (crime_data['Hour'] <= 23)).all())\n",
    "print(\"Month: \", does_column_match_pattern(crime_data, 'Month', '[0-9]{4}-[0-9]{2}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Cleaning\n",
    "\n",
    "The Philadelphia Crime Data set contains over 2.2 million rows where each row represents one 911 call about a criminal incident in the city.  Each row includes information about the date, time, police district, street, and type of incident.  There are some 2016 and 2017 incidents in the data set but there are considerable gaps in the data for those two years.  There are no apparent gaps for 2006 through 2015 so I will include only those years in the investigation.  Following this cleaning step, the data set now contains slightly over two million rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "(2035062, 14)\n"
     ]
    }
   ],
   "source": [
    "# Remove incidents from 2016 and 2017 since there are large gaps in the data\n",
    "years_included = ['2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "crime_data = crime_data[crime_data['Dispatch_Date_Time'].astype(str).str.startswith(tuple(years_included))]\n",
    "\n",
    "# How large is data set now?\n",
    "print(\"Data dimensions: \")\n",
    "print(crime_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are less than 1000 entries which do not contain a crime classification and corresponding description.  Since I wish to measure the crime type, I will remove these rows from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "(2035062, 14)\n"
     ]
    }
   ],
   "source": [
    "# Exclude rows without a crime type or description\n",
    "crime_data.dropna(subset=['UCR_General', 'Text_General_Code'], inplace = True)\n",
    "\n",
    "# How large is data set now?\n",
    "print(\"Data dimensions: \")\n",
    "print(crime_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I looked at the distinct police districts represented in the dataset, I compared the district numbers to a map of the Philadelphia Police Districts.  There are four districts which no longer exist due to mergers.  The 4th district was merged into the 3rd district.  The 23rd was absored by the 22nd district.  The 77th was absorbed by the 12th district.  Meanwhile, the 92nd district is now part of the 14th district.  The same merger will occur in my data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before district merger:\n",
      "Dc_Dist\n",
      "1      44775\n",
      "2     106155\n",
      "3      76989\n",
      "4      29197\n",
      "5      28582\n",
      "6      87384\n",
      "7      40653\n",
      "8      67759\n",
      "9      76028\n",
      "12    121702\n",
      "14    109950\n",
      "15    167387\n",
      "16     66705\n",
      "17     69817\n",
      "18     99031\n",
      "19    124281\n",
      "22    113371\n",
      "23     27278\n",
      "24    146134\n",
      "25    138099\n",
      "26     78988\n",
      "35    119559\n",
      "39     86675\n",
      "77      6936\n",
      "92      1627\n",
      "Name: Dc_Dist, dtype: int64\n",
      "\n",
      "\n",
      "After district merger:\n",
      "Dc_Dist\n",
      "1      44775\n",
      "2     106155\n",
      "3     106186\n",
      "5      28582\n",
      "6      87384\n",
      "7      40653\n",
      "8      67759\n",
      "9      76028\n",
      "12    128638\n",
      "14    111577\n",
      "15    167387\n",
      "16     66705\n",
      "17     69817\n",
      "18     99031\n",
      "19    124281\n",
      "22    140649\n",
      "24    146134\n",
      "25    138099\n",
      "26     78988\n",
      "35    119559\n",
      "39     86675\n",
      "Name: Dc_Dist, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check before merger\n",
    "print(\"Before district merger:\")\n",
    "dc_dist_distinct = crime_data.groupby('Dc_Dist')['Dc_Dist'].count()\n",
    "print(dc_dist_distinct)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Merge districts\n",
    "crime_data.loc[crime_data.Dc_Dist == 4, 'Dc_Dist'] = 3\n",
    "crime_data.loc[crime_data.Dc_Dist == 23, 'Dc_Dist'] = 22\n",
    "crime_data.loc[crime_data.Dc_Dist == 77, 'Dc_Dist'] = 12\n",
    "crime_data.loc[crime_data.Dc_Dist == 92, 'Dc_Dist'] = 14\n",
    "\n",
    "# Check the result of the merger\n",
    "print(\"After district merger:\")\n",
    "dc_dist_distinct = crime_data.groupby('Dc_Dist')['Dc_Dist'].count()\n",
    "print(dc_dist_distinct)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The **Location_Block** field is not consistent so there will be some cleaning needed to ensure the same pattern in the data.  The cleanup code will ensure that the locations are specified as either a block or an intersection.  A block is denoted as _2400 BLOCK FRONT STREET_.  An intersection fits the pattern _RHAWN ST / COTTMAN AV_.  The street types are standarized to ensure consistency as much as practical.  For example, all _ROAD_ street types are changed to _RD_.  Rows with empty or invalid locations (such as a single word) are removed from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "(2034110, 14)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regular expressions\n",
    "block_re = '^(\\d+)(\\s)BLOCK(\\s)(.+)(\\s)(\\w+)$'\n",
    "block_missed_re = '^(\\d+)(\\s)(.+)(\\s)(\\w+)$'\n",
    "block_street_missed_re = '^(\\d+)(.+)$'\n",
    "street_missed_re = '^(\\d+)(\\s)BLOCK(\\s)(.+)$'\n",
    "int_amp_re = '^(.+)(\\s)(\\w+)(\\s)&(.+)(\\s)(\\w+)$'\n",
    "int_slash_re = '^(.+)(\\s)(\\w+)(\\s)(\\/)(.+)(\\s)(\\w+)$'\n",
    "\n",
    "# Location formatting function\n",
    "def format_location_data(item):\n",
    "    ''' Format data set's location block column (invoked via apply method)\n",
    "        item = one item from location block column\n",
    "    '''  \n",
    "    # Initial trimming\n",
    "    item = item.strip()\n",
    "    \n",
    "    # Remove extraneous first characters\n",
    "    if (item[0] == '`' or item[0] == '/'):\n",
    "        item = item[1:]\n",
    "        \n",
    "    # Remove item with no whitespace\n",
    "    if (len(item.split()) == 1):\n",
    "        item = 'None'\n",
    "    \n",
    "    # We will not look at establishment, only the street\n",
    "    if ('@' in item):\n",
    "        at_index = item.index('@')\n",
    "        item = item[at_index+1:] # also remove '@'\n",
    "        item = item.strip()\n",
    "\n",
    "    # BLOCK and SLASH patterns accepted at this stage\n",
    "    if (re.fullmatch(block_re, item) or re.fullmatch(int_slash_re, item)):\n",
    "        pass\n",
    "    \n",
    "    elif (re.fullmatch(street_missed_re, item)):\n",
    "        # Add default street type\n",
    "        item += \" ST\"\n",
    "    \n",
    "    elif (re.fullmatch(block_missed_re, item)):\n",
    "        # Add BLOCK after number\n",
    "        tokens = item.split()\n",
    "        item = tokens[0] + \" BLOCK\"\n",
    "        \n",
    "        # Reconstruct string\n",
    "        for i in range(1,len(tokens)):\n",
    "            item = item + \" \" + tokens[i]\n",
    "    \n",
    "    elif (re.fullmatch(block_street_missed_re, item)):\n",
    "        # Add BLOCK after number\n",
    "        tokens = item.split()\n",
    "        item = tokens[0] + \" BLOCK\"\n",
    "        \n",
    "        # Reconstruct string\n",
    "        for i in range(1,len(tokens)):\n",
    "            item = item + \" \" + tokens[i]\n",
    "            \n",
    "        # Add default street type\n",
    "        item += \" ST\"\n",
    "    \n",
    "    elif (re.fullmatch(int_amp_re, item)):\n",
    "        # Replace ampersand\n",
    "        item = item.replace(\"&\", \"/\")\n",
    "    \n",
    "    return item\n",
    "\n",
    "# Street Types\n",
    "STREET_TYPE_EXPECTED = [\"AV\", \"BLVD\", \"CIR\", \"CT\", \"DR\", \"LN\", \"PKWY\", \"PL\", \"RD\", \"ROW\", \"ST\", \"TER\", \"WAY\"]\n",
    "\n",
    "STREET_TYPE_MAPPING = { \"AVE\": \"AV\",\n",
    "                        \"AVENUE\": \"AV\",\n",
    "                        \"AVE\": \"AV\",\n",
    "                        \"BLD\": \"BLVD\",\n",
    "                        \"BLV\": \"BLVD\",\n",
    "                        \"BDV\": \"BLVD\",\n",
    "                        \"BOULEVARD\": \"BLVD\",\n",
    "                        \"CI\": \"CIR\",\n",
    "                        \"CIRCLE\": \"CIR\",\n",
    "                        \"CRT\": \"CT\",\n",
    "                        \"COURT\": \"CT\",\n",
    "                        \"DRIVE\": \"DR\",\n",
    "                        \"LANE\": \"LN\",\n",
    "                        \"PKY\": \"PKWY\",\n",
    "                        \"PWY\": \"PKWY\",\n",
    "                        \"PARKWAY\": \"PKWY\",\n",
    "                        \"PLA\": \"PL\",\n",
    "                        \"PLACE\": \"PL\",\n",
    "                        \"RDS\": \"RD\",\n",
    "                        \"ROAD\": \"RD\",\n",
    "                        \"STR\": \"ST\",\n",
    "                        \"STT\": \"ST\",\n",
    "                        \"STREET\": \"ST\",\n",
    "                        \"TRCE\": \"TER\",\n",
    "                        \"WA\": \"WAY\"\n",
    "                      }\n",
    "\n",
    "# Street Type Formatting function\n",
    "def format_street_type(item):\n",
    "    ''' Format street type in data set's location block column (invoked via apply method)\n",
    "        This function is invoked agter format_location_data\n",
    "        item = one item from location block column\n",
    "    '''\n",
    "    # BLOCK pattern\n",
    "    if (re.fullmatch(block_re, item)):\n",
    "        tokens = item.split()\n",
    "    \n",
    "        # Check the hundred block\n",
    "        if (tokens[0].isdigit()):\n",
    "            # Check the hundred block\n",
    "            tokens = item.split()\n",
    "            block_num = int(tokens[0])\n",
    "            block_num = (block_num // 100) * 100\n",
    "            item = str(block_num)\n",
    "        else:\n",
    "            item = tokens[0]\n",
    "            \n",
    "        # Keep BLOCK and name of street\n",
    "        item = item + \" \" + tokens[1] + \" \" + tokens[2]\n",
    "    \n",
    "        # Check remaining tokens for street type\n",
    "        for i in range(3, len(tokens)):\n",
    "            if (tokens[i] in STREET_TYPE_MAPPING):\n",
    "                item = item + \" \" + STREET_TYPE_MAPPING[tokens[i]]\n",
    "                break\n",
    "            elif (tokens[i] in STREET_TYPE_EXPECTED):\n",
    "                item = item + \" \" + tokens[i]\n",
    "                break\n",
    "            else:\n",
    "                item = item + \" \" + tokens[i]\n",
    "    \n",
    "    # SLASH pattern\n",
    "    elif (re.fullmatch(int_slash_re, item)):\n",
    "        street_names = item.split('/')\n",
    "        \n",
    "        # Go through each part\n",
    "        this_part = \"\"\n",
    "        for i in range(0, len(street_names)):\n",
    "            if (i>0):\n",
    "                # Previous part\n",
    "                item = this_part + \" / \"\n",
    "                this_part = \"\"\n",
    "                \n",
    "            # Initial trimming\n",
    "            street_names[i].strip()\n",
    "            \n",
    "            # Split this part\n",
    "            tokens = street_names[i].split()\n",
    "            \n",
    "            # Empty item?\n",
    "            if (len(tokens) == 0):\n",
    "                break\n",
    "        \n",
    "            # Keep first token (for case like STREET RD)\n",
    "            this_part = this_part + tokens[0]\n",
    "            \n",
    "            # Check remaining tokens for street type\n",
    "            for j in range(1, len(tokens)):\n",
    "                if (tokens[j] in STREET_TYPE_MAPPING):\n",
    "                    this_part = this_part + \" \" + STREET_TYPE_MAPPING[tokens[j]]\n",
    "                    break\n",
    "                elif (tokens[j] in STREET_TYPE_EXPECTED):\n",
    "                    this_part = this_part + \" \" + tokens[j]\n",
    "                    break\n",
    "                else:\n",
    "                    this_part = this_part + \" \" + tokens[j]\n",
    "            \n",
    "        # Last part\n",
    "        item = item + this_part\n",
    "\n",
    "    return item\n",
    "\n",
    "# Standardize the location data\n",
    "crime_data['Location_Block'] = crime_data['Location_Block'].apply(format_location_data)\n",
    "\n",
    "# Dump items with no information\n",
    "crime_data = crime_data[crime_data['Location_Block'] != 'None']\n",
    "\n",
    "# Standardize street type\n",
    "crime_data['Location_Block'] = crime_data['Location_Block'].apply(format_street_type)\n",
    "\n",
    "# How large is data set now?\n",
    "print(\"Data dimensions: \")\n",
    "print(crime_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more cleaning step that must take place.  If multiple people placed a 911 call for the same incident, the incident is recorded twice in this data set.  These rows must be combined so that each incident is counted only once regardless of the number of 911 callers.  After removing such duplicates, the number of rows in the data set is about 1.95 million, or 80000 fewer than the previous iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "(1953458, 4)\n"
     ]
    }
   ],
   "source": [
    "# Try groupby to remove duplicates\n",
    "crime_data = crime_data[['Dispatch_Date_Time', 'Location_Block', 'UCR_General', 'Text_General_Code']].drop_duplicates()\n",
    "\n",
    "# Sort data set to better see repeat incidents\n",
    "crime_data.sort_values(['Dispatch_Date_Time'], ascending=False, inplace=True)\n",
    "\n",
    "# How large is data set now?\n",
    "print(\"Data dimensions: \")\n",
    "print(crime_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Data cleaning is finished; plot and analyze data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
